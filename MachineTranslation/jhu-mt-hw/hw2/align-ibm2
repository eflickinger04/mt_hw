#!/usr/bin/env python
import optparse
import sys
from collections import defaultdict
import math

optparser = optparse.OptionParser()
optparser.add_option("-d", "--data", dest="train", default="data/hansards", help="Data filename prefix (default=data)")
optparser.add_option("-e", "--english", dest="english", default="e", help="Suffix of English filename (default=e)")
optparser.add_option("-f", "--french", dest="french", default="f", help="Suffix of French filename (default=f)")
optparser.add_option("-n", "--num_sentences", dest="num_sents", default=1000, type="int", help="Number of sentences to use for training and alignment")
optparser.add_option("-i", "--iterations", dest="iterations", default=7, type="int", help="Number of EM iterations for IBM Model 2")
(opts, _) = optparser.parse_args()

f_data = "%s.%s" % (opts.train, opts.french)
e_data = "%s.%s" % (opts.train, opts.english)

sys.stderr.write("Loading data...")
bitext = [[sentence.strip().split() for sentence in pair] for pair in zip(open(f_data), open(e_data))][:opts.num_sents]
sys.stderr.write("Done\n")

# initialize translation probabilities (t(f|e))
translation_prob = defaultdict(lambda: defaultdict(lambda: 1.0))  # P(f|e)

#initialize alignment probabilities (p_a(j|i, l_e, l_f)) 
alignment_prob = defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: defaultdict(lambda: 1.0))))

# IBM2 algorithm
sys.stderr.write("Training IBM Model 2 using EM...\n")
for iteration in range(opts.iterations):
    sys.stderr.write(f"Iteration {iteration + 1}/{opts.iterations}...\n")

    #initialize counts
    count_e_given_f = defaultdict(float)  # C(e|f)
    total_f = defaultdict(float)          # Total(f)
    count_a = defaultdict(float)          # C(j|i, l_e, l_f)
    total_a = defaultdict(float)          # Total(i, l_e, l_f)

    #expectation
    for (f, e) in bitext:
        l_f = len(f)
        l_e = len(e)
        
        # normalization
        total_e_given_f = defaultdict(float)
        for i, e_j in enumerate(e):
            total_e_given_f[e_j] = sum(translation_prob[f_i][e_j] * alignment_prob[i][j][l_e][l_f] for j, f_i in enumerate(f))


        for j, f_i in enumerate(f):
            for i, e_j in enumerate(e):
                count = translation_prob[f_i][e_j] * alignment_prob[i][j][l_e][l_f] / total_e_given_f[e_j]
                count_e_given_f[(f_i, e_j)] += count
                total_f[f_i] += count
                count_a[(i, j, l_e, l_f)] += count
                total_a[(i, l_e, l_f)] += count

    # maximize
    for (f_i, e_j), count in count_e_given_f.items():
        translation_prob[f_i][e_j] = count / total_f[f_i]

    for (i, j, l_e, l_f), count in count_a.items():
        alignment_prob[i][j][l_e][l_f] = count / total_a[(i, l_e, l_f)]

sys.stderr.write("IBM Model 2 training complete.\n")

# align  using the P(translation) & P(alignment)
sys.stderr.write("Aligning sentences...\n")
for (f, e) in bitext:
    alignment = []
    l_f = len(f)
    l_e = len(e)
    for i, f_i in enumerate(f):
        best_prob = 0.0
        best_j = 0
        for j, e_j in enumerate(e):
            prob = translation_prob[f_i][e_j] * alignment_prob[j][i][l_e][l_f]
            if prob > best_prob:
                best_prob = prob
                best_j = j
        alignment.append(f"{i}-{best_j}")
    sys.stdout.write(" ".join(alignment) + "\n")
sys.stderr.write("Alignment complete.\n")
