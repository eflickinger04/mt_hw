#!/usr/bin/env python
import heapq
import sys
import models
import math
import re
from collections import namedtuple

class Hypothesis:
    def __init__(self, score, seq_translated, source_parts_translated, end):
        self.score = score
        if source_parts_translated is None:
            self.source_parts_translated = []
        else:
            self.source_parts_translated = source_parts_translated
        if seq_translated is None:
            self.seq_translated = []
        else:
            self.seq_translated = seq_translated
        self.end = end

def tokenize(text):
    return re.findall(r"\w+|[^\w\s]", text, re.UNICODE)

def beam_search_decoder(french, tm, lm, beam_width, threshold):
    # Initialize the beam with the initial hypothesis and LM state
    initial_h = Hypothesis(0.0, [], [0] * len(french), 0)
    lm_initial_state = ('<s>',)  
    beam = [(initial_h, lm_initial_state)]
    
    # Go through the french input
    for _ in range(len(french)):
        possibilities = []
        for h, current_lm_state in beam:
            for i in range(len(french)):
                for j in range(i + 1, len(french) + 1):
                    # if the phrase hasn't been translated yet
                    if sum(h.source_parts_translated[i:j]) == 0:
                        phrase = tuple(french[i:j])
                        if phrase in tm:
                            translations = tm[phrase]
                            for translation, phrase_probability in translations:
                                new_source_parts_translated = h.source_parts_translated.copy()
                                for k in range(i, j):
                                    new_source_parts_translated[k] = 1
                                if phrase_probability > 0:
                                    phrase_log_probability = math.log(phrase_probability)
                                else:
                                    phrase_log_probability = -math.inf
                                
                                # Look at the alr translated sequence
                                if h.seq_translated:
                                    prev_word = (h.seq_translated[-1],)
                                else:
                                    prev_word = ('<s>',) 
                                
                                # Use tokenize to handle punctuation
                                translation_tokens = tokenize(translation)
                                translation_tuple = tuple(translation_tokens)
                                
                                if translation_tuple:
                                    curr_word = translation_tuple[0]
                                    result = lm.score(prev_word, (curr_word,))
                                    
                                    if isinstance(result, tuple):
                                        if len(result) == 2:
                                            new_lm_state, lm_score = result
                                            lm_score = 0.0
                                        else:
                                            new_lm_state = current_lm_state
                                            lm_score = 0.0
                                    else:
                                        new_lm_state = current_lm_state
                                        lm_score = 0.0
                                else:
                                    lm_score = 0.0
                                    new_lm_state = current_lm_state
                                
                                # Total score
                                total_score = h.score + phrase_log_probability + lm_score
                                new_seq_translated = h.seq_translated + translation_tokens
                                new_end = j
                                
                                # New hypothesis
                                new_hypothesis = Hypothesis(total_score, new_seq_translated, new_source_parts_translated, new_end)
                                
                                # Add the new hypothesis/lm state
                                possibilities.append((new_hypothesis, new_lm_state))
        
        # If no possibilites, end it 
        if not possibilities:
            break
        
        # Sort by descending order in score
        possibilities.sort(key=lambda x: x[0].score, reverse=True)
        
        # Threshold pruning
        best_score = possibilities[0][0].score
        pruned_translations = [h for h in possibilities if h[0].score >= best_score * threshold]
        
        # Add the pruned hypothesis
        beam = pruned_translations[:beam_width]

    best_hypothesis = max(beam, key=lambda x: x[0].score)[0]
    return best_hypothesis.seq_translated

def main():
    if len(sys.argv) != 3:
        sys.stderr.write("Usage: python3 beam_search_decode <input_file> <output_file>\n")
        sys.exit(1)
    
    tm = models.TM('data/tm', k=3)  
    lm = models.LM('data/lm')       
    # Read input French sentences
    input_file = sys.argv[1]
    output_file = sys.argv[2]
    with open(input_file, 'r') as f:
        french = [line.strip().split() for line in f.readlines() if line.strip()]
    
    with open(output_file, 'w') as output:
        for idx, sentence in enumerate(french, 1):
            print(f"Translating sentence {idx}: {' '.join(sentence)}")
            translation = beam_search_decoder(sentence, tm, lm, beam_width=5, threshold=0.85)
            translation_sentence = ' '.join(translation)
            output.write(translation_sentence + "\n")
            print(f"Completed translation for: {' '.join(sentence)}")
    print(f"Translations written to {output_file}")

if __name__ == "__main__":
    main()

